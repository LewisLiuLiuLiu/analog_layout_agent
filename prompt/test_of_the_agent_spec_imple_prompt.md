你现在的角色是：负责测试体系设计的工程师。

【输入上下文】
- 我会提供《opamp_layout_gen_Agent_proposal.md》全文。
- proposal 第 3.4 节已经描述了单元测试、集成测试、回归测试的大致方向。
- 你的任务是：**写出“测试与评估框架”章节的工程实现方案**。

【输出目标】
形成一章从测试类型、测试用例设计、执行方式到通过标准的完整测试方案，用于指导后续测试代码和 CI 流水线实现。

【输出内容结构要求】
建议结构如下：
1. 测试目标与范围
   - 明确测试关注的指标：
     - Skill 正确性（几何/逻辑）
     - MVP 场景端到端 DRC=0
     - 面积/匹配度等指标不退化
   - 明确不在当前测试范围内的内容（例如复杂 PDK、超大规模电路）。

2. 单元测试（Skill 级）
   - 指定需要覆盖的 Skill 列表（至少所有 P0/P1）。
   - 为每类 Skill 提供典型测试用例设计原则：
     - 输入组合、边界条件、异常参数。
   - 说明期望检查的内容：
     - 输出结构字段完整性、几何对象数量和关系、DRC 报告格式等。
   - 说明建议使用的测试框架/工具（例如某种语言上的测试框架）。

3. 集成测试（MVP 场景端到端）
   - 定义一组标准 MVP 网表和 DRC 规则输入集合。
   - 说明如何运行完整流水线：
     - 通过 Orchestrator 调用 Agent → 生成版图 → 运行 DRC → 评估。
   - 定义通过标准：
     - DRC 错误数 = 0
     - 面积、匹配度在某个阈值内（可与基线版本比较）。

4. 回归测试与基准用例集
   - 设计“基准用例集”的构成：不同拓扑/参数规格的 opamp 网表。
   - 说明回归测试触发时机（如每次重要修改/发布前）。
   - 定义回归报告格式：
     - 将关键指标（DRC 错误、面积、匹配度）按版本对比展示。

5. 自动化与 CI 集成
   - 说明如何在 CI 流水线中集成：
     - 哪些测试在每次提交时运行，哪些只在 nightly / release 前运行。
   - 描述测试环境准备（依赖服务：KLayout、向量库、LLM mock 或 stub）。

6. 评估与指标监控
   - 进一步定义量化指标：
     - 通过率、平均 DRC 错误数、平均面积、匹配度分数。
   - 建议将这些指标记录到日志或监控系统中，支持长期跟踪。

【技术深度要求】
- 要写到“**测试工程师可以直接据此写测试代码和 CI 配置**”的程度：
  - 测试用例需要有代表性的示例输入/输出描述。
  - 对每种测试的通过/失败条件要明确。

【与整体项目关联性要求】
- 测试目标要直接对应 proposal 中的 MVP 定义和自我评估指标（DRC、面积、匹配度）。
- 测试章节中引用的模块和接口应与其他实现章节使用的名称保持一致。